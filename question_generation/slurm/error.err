Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2-large and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias', 'h.12.attn.masked_bias', 'h.13.attn.masked_bias', 'h.14.attn.masked_bias', 'h.15.attn.masked_bias', 'h.16.attn.masked_bias', 'h.17.attn.masked_bias', 'h.18.attn.masked_bias', 'h.19.attn.masked_bias', 'h.20.attn.masked_bias', 'h.21.attn.masked_bias', 'h.22.attn.masked_bias', 'h.23.attn.masked_bias', 'h.24.attn.masked_bias', 'h.25.attn.masked_bias', 'h.26.attn.masked_bias', 'h.27.attn.masked_bias', 'h.28.attn.masked_bias', 'h.29.attn.masked_bias', 'h.30.attn.masked_bias', 'h.31.attn.masked_bias', 'h.32.attn.masked_bias', 'h.33.attn.masked_bias', 'h.34.attn.masked_bias', 'h.35.attn.masked_bias', 'lm_head.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/ext3/miniconda3/envs/hug/lib/python3.7/site-packages/torch/cuda/__init__.py:104: UserWarning: 
NVIDIA A100-SXM4-80GB with CUDA capability sm_80 is not compatible with the current PyTorch installation.
The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70 sm_75.
If you want to use the NVIDIA A100-SXM4-80GB GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/

  warnings.warn(incompatible_device_warn.format(device_name, capability, " ".join(arch_list), device_name))
  0%|          | 0/169 [00:00<?, ?it/s]  0%|          | 0/169 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "decode.py", line 199, in <module>
    main()
  File "decode.py", line 141, in main
    attention_mask = (~torch.eq(input_ids, PAD_ID)).int()
RuntimeError: CUDA error: no kernel image is available for execution on the device
  2%|▏         | 3/169 [2:23:20<133:01:08, 2884.75s/it]  2%|▏         | 4/169 [3:13:34<134:33:26, 2935.80s/it]  3%|▎         | 5/169 [4:12:02<143:08:06, 3141.99s/it]  4%|▎         | 6/169 [5:12:04<149:20:40, 3298.41s/it]  4%|▍         | 7/169 [6:04:12<145:55:14, 3242.68s/it]  5%|▍         | 8/169 [7:04:06<150:00:53, 3354.37s/it]  5%|▌         | 9/169 [8:03:47<152:14:26, 3425.41s/it]  6%|▌         | 10/169 [8:57:14<148:18:29, 3357.92s/it]  7%|▋         | 11/169 [9:51:33<146:02:28, 3327.52s/it]  7%|▋         | 12/169 [10:42:44<141:42:52, 3249.51s/it]  8%|▊         | 13/169 [11:37:19<141:09:28, 3257.49s/it]  8%|▊         | 14/169 [12:28:22<137:42:44, 3198.48s/it]  9%|▉         | 15/169 [13:28:05<141:47:05, 3314.45s/it]  9%|▉         | 16/169 [14:16:50<135:53:11, 3197.33s/it] 10%|█         | 17/169 [15:05:54<131:46:29, 3120.99s/it] 11%|█         | 18/169 [16:00:26<132:49:13, 3166.58s/it] 11%|█         | 19/169 [16:48:12<128:10:13, 3076.09s/it] 12%|█▏        | 20/169 [17:36:42<125:15:13, 3026.26s/it] 12%|█▏        | 21/169 [18:36:07<131:03:37, 3187.96s/it] 13%|█▎        | 22/169 [19:28:32<129:38:51, 3175.04s/it] 14%|█▎        | 23/169 [20:19:22<127:15:02, 3137.69s/it] 14%|█▍        | 24/169 [21:10:54<125:49:19, 3123.86s/it] 15%|█▍        | 25/169 [22:07:35<128:16:53, 3207.04s/it] 15%|█▌        | 26/169 [22:53:22<121:54:41, 3069.10s/it] 16%|█▌        | 27/169 [23:48:50<124:07:18, 3146.75s/it]slurmstepd: error: *** JOB 46244117 ON gr047 CANCELLED AT 2024-05-09T06:30:54 DUE TO TIME LIMIT ***
